{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ****What is AUTOGRAD ?****"
      ],
      "metadata": {
        "id": "81JFNeU6vMKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<span style=\"color:blue\">***Autograd is core component of pytorch that provides automatic differentiation of tensor operations. It enables gradient computation, which is essential for training machine learning models using optimization algorithms like gradient descent.***</span>\n"
      ],
      "metadata": {
        "id": "RIrasUe6wpq8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSzv6ccGs0DZ",
        "outputId": "42f5c9cc-33ce-454c-96d3-a447d66e6db6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ],
      "source": [
        "def dy_dx(x):\n",
        "  return 2 * x\n",
        "\n",
        "\n",
        "dy_dx(4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "i9Ue28Be4gjU"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(4.0, requires_grad = True)\n",
        "\n",
        "\n",
        "# \"requires_grad = True\" when we use this attribute. pytorch understand that\n",
        "# we wanted to calculate gradient of \"x\". So whatever operations we will do on it\n",
        "# pytorch keep track on these operations and when we call it, it will return gradients."
      ],
      "metadata": {
        "id": "eorf5OX04uyK"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2\n",
        "print(x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTDkylpQ513Y",
        "outputId": "bda5dc5e-e659-41cd-9614-098f7b37ec67"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4., requires_grad=True) tensor(16., grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()              # Calculate gradients in backward direction."
      ],
      "metadata": {
        "id": "WJbbycyl6V5d"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad                    # return gradient without writing any function."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51-R1Aut6p4n",
        "outputId": "b2d249a8-82ff-4304-c518-4b91e4218cf4"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(8.)"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def dz_dx(x):\n",
        "  return 2 * x * math.cos(x ** 2)\n",
        "\n",
        "\n",
        "dz_dx(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOZgtswM6vIw",
        "outputId": "67600440-8dd0-49a9-da83-8241a27b16b7"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-5.466781571308061"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0, requires_grad = True)\n",
        "\n",
        "y = (x ** 2)\n",
        "\n",
        "z = torch.sin(y)\n",
        "\n",
        "z.backward()\n",
        "\n",
        "x.grad\n",
        "\n",
        "print(f\"x --> {x},   y --> {y},   z --> {z},   x_gradient --> {x.grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0el_9RjN7Zfk",
        "outputId": "6698e8b0-086b-4a1e-81c8-8900e5188bbf"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x --> 3.0,   y --> 9.0,   z --> 0.41211849451065063,   x_gradient --> -5.4667816162109375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ****AAM ZINDAGI****"
      ],
      "metadata": {
        "id": "Vf939tswZ884"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Inputs\n",
        " x = torch.tensor(6.7)                   # Input Feature\n",
        " y = torch.tensor(0.0)                   # True Label (Binary)\n",
        "\n",
        " w = torch.tensor(1.0)                   # Weight\n",
        " b = torch.tensor(0.0)                   # Bias"
      ],
      "metadata": {
        "id": "BZ_gtydb8A2V"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Crosss-Entropy Loss for Scalar\n",
        "\n",
        "def binary_cross_entropy(prediction, target):\n",
        "  epsilon = 1e-8                          # To prevent log(0)\n",
        "  prediction = torch.clamp(prediction, epsilon, 1 - epsilon)\n",
        "  return - (target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))"
      ],
      "metadata": {
        "id": "quYsouVjT7qU"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward Pass\n",
        "z = w * x + b                             # Weighted Sum (Linear part)\n",
        "y_pred = torch.sigmoid(z)                 # Predicted Probability\n",
        "\n",
        "# Calculate Binary_cross_Entropy\n",
        "loss = binary_cross_entropy(y_pred, y)"
      ],
      "metadata": {
        "id": "PKaiRIr2U0kK"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Derivatives:\n",
        "# 1. dL/d(y_pred) : Loss with respect to predition (y_pred)\n",
        "dloss_y_pred = (y_pred - y) / (y_pred * (1 - y_pred))\n",
        "\n",
        "# 2. dy_pred/dz : prediction (y_pred) with respect to z (sigmoid derivative)\n",
        "dy_pred_dz = y_pred * (1 - y_pred)\n",
        "\n",
        "# 3. dz/dw and dz/db: z with respect to w & b\n",
        "dz_dw = x                                  # dz/dw = x\n",
        "dz_db = 1                                  # dz/db = 1 (bias contributes directly to z)\n",
        "\n",
        "dL_dw = dloss_y_pred * dy_pred_dz * dz_dw\n",
        "dL_db = dloss_y_pred * dy_pred_dz * dz_db"
      ],
      "metadata": {
        "id": "2WhKak3xVfDE"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Manual Gradient of loss w.r.t. weight (dw) --> {dL_dw}')\n",
        "print(f'\\nManual Gradient of loss w.r.t. bias (db) --> {dL_db}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo7j8r7qZCkP",
        "outputId": "f4d408cf-650f-4209-cf55-39587b3d9292"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Gradient of loss w.r.t. weight (dw) --> 6.691762447357178\n",
            "\n",
            "Manual Gradient of loss w.r.t. bias (db) --> 0.998770534992218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ****MENTOS ZINDAGI****"
      ],
      "metadata": {
        "id": "kxVp1zJBaDVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(6.7)\n",
        "y = torch.tensor(0.0)"
      ],
      "metadata": {
        "id": "-cEZ4DJtZvwG"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.tensor(1.0, requires_grad = True)\n",
        "b = torch.tensor(0.0, requires_grad = True)"
      ],
      "metadata": {
        "id": "ZmTRVKMSaR67"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoSQgyHWacwW",
        "outputId": "ec49f51b-e8fc-43c3-9194-a23f10cc3aa2"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., requires_grad=True) tensor(0., requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = w * x + b\n",
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0F1YbTYahlX",
        "outputId": "1823c5d1-1892-407c-c3e4-5a351b15552e"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.7000, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = torch.sigmoid(z)\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Cfe-9c9am3R",
        "outputId": "a42d8029-3633-4973-fdad-efb642071d79"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9988, grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy(y_pred, y)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-W3EW0oa37j",
        "outputId": "a362f54b-f795-49a2-ac2c-22099b3a23e4"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.7012, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROz1-TqDbH_q",
        "outputId": "7349fbfe-6e69-4763-d0b9-a4810bf95640"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.6918)\n",
            "tensor(0.9988)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 5.0, 7.6], requires_grad = True)\n",
        "\n",
        "y = (x ** 2).mean()\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiN_YrPJbonw",
        "outputId": "3ef77f53-17d6-4653-d4f1-f39b9859bc2f"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(27.9200, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtYclvQWcmKE",
        "outputId": "ac6a1d82-3d97-40cd-b01d-f6e0b1f5ee95"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.6667, 3.3333, 5.0667])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ****Clearing Gradient****"
      ],
      "metadata": {
        "id": "-9b63CDSebWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clearing Grad\n",
        "\n",
        "x = torch.tensor(3.0, requires_grad = True)\n",
        "print('x --> ', x)\n",
        "\n",
        "y = x ** 2\n",
        "print('y --> ', y)\n",
        "\n",
        "y.backward()\n",
        "print('x_grad --> ', x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YQuCCwCcrEs",
        "outputId": "1bdeb45a-d975-4186-b49e-f04cbec2d809"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x -->  tensor(3., requires_grad=True)\n",
            "y -->  tensor(9., grad_fn=<PowBackward0>)\n",
            "x_grad -->  tensor(6.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2\n",
        "y.backward()\n",
        "print(x.grad)\n",
        "x.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGzMjv0Ndgxc",
        "outputId": "c0bb120c-21e6-4f14-e459-63175691caeb"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(12.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ****Disable Gradient Tracking****"
      ],
      "metadata": {
        "id": "R6PCIKV1ejEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Requires_grad_(False)\n",
        "2.   detach()\n",
        "3.   torch.no_grad()\n",
        "\n"
      ],
      "metadata": {
        "id": "v43gECPIgmCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(5.0, requires_grad = True)\n",
        "print('x --> ', x)\n",
        "\n",
        "y = x ** 2\n",
        "print('y --> ', y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdIc4koid4kb",
        "outputId": "4a2b2216-6c33-4349-f6a5-8da4cc0c0601"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x -->  tensor(5., requires_grad=True)\n",
            "y -->  tensor(25., grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgOUsqSoe85x",
        "outputId": "2173b870-709f-41f1-cb91-16e0b3398597"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10.)"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 1 --> requires_grad_(False)\n",
        "\n",
        "x.requires_grad_(False)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVVByMjogcJw",
        "outputId": "f866194f-7e0a-40f0-84f1-2675cb14805c"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.)"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 2 --> detach()\n",
        "\n",
        "n = torch.tensor(10.0, requires_grad = True)\n",
        "print('n --> ', n)\n",
        "\n",
        "k = n.detach()\n",
        "\n",
        "z = n ** 2\n",
        "print('z --> ', z)\n",
        "\n",
        "z1 = k ** 2\n",
        "print('z1 --> ', z1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2AbjDesg79p",
        "outputId": "515a7c63-9ff2-4649-a33f-a6fdc09dfa59"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n -->  tensor(10., requires_grad=True)\n",
            "z -->  tensor(100., grad_fn=<PowBackward0>)\n",
            "z1 -->  tensor(100.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z.backward()"
      ],
      "metadata": {
        "id": "AlnvqV2JjFv-"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z1.backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "LWt5eZbnjUQN",
        "outputId": "c2b2153e-baad-4099-b430-372127a4e547"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-759771097.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 3 --> torch.no_grad()\n",
        "\n",
        "m = torch.tensor(4.2, requires_grad = True)\n",
        "print('m --> ', m)\n",
        "\n",
        "with torch.no_grad():\n",
        "  l = m ** 2\n",
        "\n",
        "print('l --> ', l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0zjUh_-je0E",
        "outputId": "8d47315f-ee19-4e0a-c5b4-e9c1e4ee7658"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "m -->  tensor(4.2000, requires_grad=True)\n",
            "l -->  tensor(17.6400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l.backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "V3VluKkwkgZ5",
        "outputId": "20509a8c-0487-44ea-fe8c-36ab622e7336"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2213585194.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    }
  ]
}